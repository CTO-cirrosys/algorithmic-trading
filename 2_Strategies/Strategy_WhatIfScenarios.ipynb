{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to perform what-if scenarios for trading strategies with Amazon FinSpace\n",
    "\n",
    "This notebook will: \n",
    "\n",
    "1. Create and connect to a FinSpace managed cluster\n",
    "2. Load the data view selected in FinSpace web application into a Spark DataFrame and access data using Spark SQL\n",
    "3. Train a machine learning model with Spark ML\n",
    "4. Run multiple what-if trading strategies\n",
    "\n",
    "Before executing the notebook: \n",
    "1. Select the FinSpace PySpark Kernel in the top right corner of this notebook\n",
    "2. Wait less than 5 minutes until the FinSpace PySpark Kernel is available \n",
    "\n",
    "\n",
    "## 1. Connect to FinSpace Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "from aws.finspace.cluster import FinSpaceClusterManager\n",
    "\n",
    "# if this was already run, no need to run again\n",
    "if 'finspace_clusters' not in globals():\n",
    "    finspace_clusters = FinSpaceClusterManager()\n",
    "    finspace_clusters.auto_connect()\n",
    "else:\n",
    "    print(f'connected to cluster: {finspace_clusters.get_connected_cluster_id()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data view into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####----------------------------------------------------------\n",
    "##### REPLACE WITH CORRECT IDS!\n",
    "##### Dataset: \"US Equity Time-Bar Summary - 1 min, 14 Symbols - Sample\"\n",
    "#####----------------------------------------------------------\n",
    "dataset_id = \"\"\n",
    "data_view_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Dataset\n",
    "FinSpace API to get the dataset as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws.finspace.analytics import FinSpaceAnalyticsManager\n",
    "finspace_analytics = FinSpaceAnalyticsManager(spark = spark)\n",
    "\n",
    "df = finspace_analytics.read_data_view(dataset_id = dataset_id, data_view_id = data_view_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the DataFrame's Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Dataset\n",
    "Sample a few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query data and prepare features for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from aws.finspace.timeseries.spark.analytics import *\n",
    "from aws.finspace.timeseries.spark.windows import *\n",
    "\n",
    "from aws.finspace.timeseries.spark.util import string_to_timestamp_micros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please adjust date range as needed  \n",
    "sDate = dt.datetime(2020, 1, 1)\n",
    "eDate = dt.datetime(2020, 2, 28)\n",
    "\n",
    "df = ( df.filter(df.eventtype == \"TRADE NB\").filter( df.date.between(sDate, eDate) ).filter(df.ticker == \"AMZN\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the sets of values\n",
    "partitionList = [\"ticker\", \"eventtype\"]\n",
    "\n",
    "tenor = 10\n",
    "numStd = 2\n",
    "\n",
    "timeCol  = 'end'\n",
    "priceCol = 'vwap'\n",
    "highCol  = 'high'\n",
    "lowCol   = 'low'\n",
    "volCol   = 'volume'\n",
    "\n",
    "emaDef = exponential_moving_average(tenor, timeCol, priceCol)\n",
    "\n",
    "#Example: use MACD instead of simpler EMA\n",
    "#macdDef = moving_average_converge_diverge_hist( 12, 26, 9, timeCol, priceCol ),\n",
    "\n",
    "df = compute_analytics_on_features(df, \"exponential_moving_average\", emaDef, partition_col_list = partitionList)\n",
    "\n",
    "# will be working with the once calculated, lets cache it\n",
    "df = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futureDF = df.withColumn(\"futurestart\", df.start + F.expr('INTERVAL 5 MINUTES'))\n",
    "futureDF = futureDF.withColumnRenamed(\"vwap\", \"label\")\n",
    "futureDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're adding a new column that contains volume weighted average price and \"move\" the values ahead of 5 min (5 time period) and use this as label\n",
    "df = df.alias('df')\n",
    "futureDF = futureDF.alias('futureDF')\n",
    "\n",
    "fullDF = df.join(futureDF, df.start == futureDF.futurestart).select('df.*', 'futureDF.label', 'futureDF.futurestart', 'futureDF.start')\n",
    "fullDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a final check of our features before using them for model training\n",
    "pd.DataFrame(fullDF.take(3), columns=fullDF.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train multiple RandomForest models with different parameters via Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from time import *\n",
    "\n",
    "# Take only subset without null\n",
    "df = fullDF.collect()[50:100]\n",
    "# Take full dataset\n",
    "#df = fullDF.collect()[50:]\n",
    "\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "feature_list = [\"activity_count\", \"vwap\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"exponential_moving_average\"]\n",
    "        \n",
    "assembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor().setFeaturesCol(\"features\").setLabelCol(\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "#hyperparameters values: numTrees start, numTrees stop, numTrees num, maxDepth start, maxDepth stop, maxDepth num, numFolds\n",
    "set1=[ 5, 25, 3, 5, 25, 3, 3]\n",
    "set2=[ 10, 50, 3, 5, 25, 3, 9]\n",
    "set3=[ 5, 25, 3, 5, 25, 3, 9]\n",
    "\n",
    "params=[set1, set2, set3]\n",
    "models=[]\n",
    "\n",
    "for i in params:\n",
    "\n",
    "    print(\"set\" + str(i) )\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = i[0], stop = i[1], num = i[2])]) \\\n",
    "        .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = i[3], stop = i[4], num = i[5])]) \\\n",
    "        .build()\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=RegressionEvaluator(),\n",
    "                              numFolds=i[6])\n",
    "\n",
    "    (trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    starttime = time()\n",
    "\n",
    "    m=crossval.fit(trainingData)\n",
    "    \n",
    "    models.append(m)\n",
    "    \n",
    "    endtime = time()\n",
    "    trainingtime = endtime - starttime\n",
    "    print(\"Training time: %.3f seconds\" % (trainingtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install backtesting library and define trading strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"backtrader\")"
   ]
  },
  {
   "source": [
    "Define helper classes for using the backtesting library. This contains code to calculate performance metrics, to read time series data from the Spark cluster, and to define a base template of strading strategy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.feeds as btfeeds\n",
    "import backtrader.analyzers as btanalyzers\n",
    "from backtrader.feed import DataBase\n",
    "from backtrader import date2num\n",
    "from backtrader import TimeFrame\n",
    "import os\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "# More documentation about backtrader: https://www.backtrader.com/\n",
    "\n",
    "class AlgoStrategy():\n",
    "    \n",
    "    def __init__(self,strategy):       \n",
    "        self.cerebro = bt.Cerebro()        \n",
    "        strategy.init_broker(self.cerebro.broker)\n",
    "        data=strategy.add_data(self.cerebro)\n",
    "        strategy.data=data\n",
    "       \n",
    "        self.cerebro.addstrategy(strategy)\n",
    "        \n",
    "        self.portfolioStartValue=self.cerebro.broker.getvalue()                            \n",
    "        self.cerebro.addanalyzer(btanalyzers.DrawDown, _name='dd')\n",
    "        self.cerebro.addanalyzer(btanalyzers.SharpeRatio_A, _name='sharpe')\n",
    "        self.cerebro.addanalyzer(btanalyzers.SQN, _name='sqn')\n",
    "        self.cerebro.addanalyzer(btanalyzers.TradeAnalyzer, _name='ta')\n",
    "        \n",
    "    def performance(self):\n",
    "        analyzer=self.thestrat.analyzers.ta.get_analysis()\n",
    "        dd_analyzer=self.thestrat.analyzers.dd.get_analysis()\n",
    "      \n",
    "        #Get the results we are interested in\n",
    "        total_open = analyzer.total.open\n",
    "        total_closed = analyzer.total.closed\n",
    "        total_won = analyzer.won.total\n",
    "        total_lost = analyzer.lost.total\n",
    "        win_streak = analyzer.streak.won.longest\n",
    "        lose_streak = analyzer.streak.lost.longest\n",
    "        pnl_net = round(analyzer.pnl.net.total,2)\n",
    "        strike_rate=0\n",
    "        if total_closed>0:\n",
    "            strike_rate = (total_won / total_closed) * 100\n",
    "        #Designate the rows\n",
    "        h1 = ['Total Open', 'Total Closed', 'Total Won', 'Total Lost']\n",
    "        h2 = ['Strike Rate','Win Streak', 'Losing Streak', 'PnL Net']\n",
    "        h3 = ['DrawDown Pct','MoneyDown', '', '']\n",
    "        self.total_closed=total_closed\n",
    "        self.strike_rate=strike_rate\n",
    "        self.max_drawdown=dd_analyzer.max.drawdown\n",
    "        r1 = [total_open, total_closed,total_won,total_lost]\n",
    "        r2 = [('%.2f%%' %(strike_rate)), win_streak, lose_streak, pnl_net]\n",
    "        r3 = [('%.2f%%' %(dd_analyzer.max.drawdown)), dd_analyzer.max.moneydown, '', '']\n",
    "        #Check which set of headers is the longest.\n",
    "        header_length = max(len(h1),len(h2),len(h3))\n",
    "        #Print the rows\n",
    "        print_list = [h1,r1,h2,r2,h3,r3]\n",
    "        row_format =\"{:<15}\" * (header_length + 1)\n",
    "        print(\"Trade Analysis Results:\")\n",
    "        for row in print_list:\n",
    "            print(row_format.format('',*row))\n",
    "\n",
    "        analyzer=self.thestrat.analyzers.sqn.get_analysis()\n",
    "        sharpe_analyzer=self.thestrat.analyzers.sharpe.get_analysis()\n",
    "        self.sqn = analyzer.sqn\n",
    "        self.sharpe_ratio = sharpe_analyzer['sharperatio']\n",
    "        if self.sharpe_ratio is None:\n",
    "            self.sharpe_ratio=0\n",
    "        self.pnl = self.cerebro.broker.getvalue()-self.portfolioStartValue\n",
    "        print('[SQN:%.2f, Sharpe Ratio:%.2f, Final Portfolio:%.2f, Total PnL:%.2f]' % (self.sqn,self.sharpe_ratio,self.cerebro.broker.getvalue(),self.pnl))\n",
    "        \n",
    "    def run(self):\n",
    "        thestrats = self.cerebro.run()\n",
    "        self.thestrat = thestrats[0]\n",
    "        self.performance()\n",
    "\n",
    "class MyFeed(DataBase):\n",
    "    def __init__(self):\n",
    "        super(MyFeed, self).__init__()\n",
    "        self.list=testData.select(\"end\", \"activity_count\", \"vwap\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"exponential_moving_average\").collect()\n",
    "        self.n=0\n",
    "        \n",
    "        self.fromdate=self.list[0]['end']\n",
    "        self.todate=self.list[len(self.list)-1]['end']\n",
    "        self.timeframe=bt.TimeFrame.Minutes\n",
    "        print(\"from=%s,to=%s\" % (self.fromdate,self.todate))\n",
    "        \n",
    "        self.m={}\n",
    "        #print(self.list)\n",
    "        \n",
    "    def start(self):\n",
    "        # Nothing to do for this data feed type\n",
    "        pass\n",
    "\n",
    "    def stop(self):\n",
    "        # Nothing to do for this data feed type\n",
    "        pass\n",
    "    \n",
    "    def _load(self):\n",
    "        if self.n>=len(self.list):\n",
    "            return False\n",
    "        \n",
    "        r=self.list[self.n]\n",
    "        self.lines.datetime[0] = date2num(r['end'])\n",
    "        \n",
    "        self.lines.open[0] = r['open']\n",
    "        self.lines.high[0] = r['high']\n",
    "        self.lines.low[0] = r['low']\n",
    "        self.lines.close[0] = r['close']\n",
    "        self.lines.volume[0] = r['volume']\n",
    "        self.m[r['end']]=r\n",
    "        \n",
    "        self.n=self.n+1\n",
    "        return True\n",
    "        \n",
    "class StrategyTemplate(bt.Strategy):\n",
    "    \n",
    "    def __init__(self):         \n",
    "        self.lastDay=-1\n",
    "        self.lastMonth=-1\n",
    "        self.dataclose = self.datas[0].close\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_broker(broker):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_data(cerebro):\n",
    "        pass\n",
    "     \n",
    "    def next(self):\n",
    "        dt=self.datas[0].datetime.datetime(0)\n",
    "        #print(\"[NEXT]:%s:close=%s\" % (dt,self.dataclose[0]))\n",
    "        \n",
    "        #SOM\n",
    "        if self.lastMonth!=dt.month:\n",
    "            if self.lastMonth!=-1:\n",
    "                chg=self.broker.getvalue()-self.monthCash\n",
    "                #print(\"[%s] SOM:chg=%.2f,cash=%.2f\" % (dt,chg,self.broker.getvalue()))\n",
    "            self.lastMonth=dt.month\n",
    "            self.monthCash=self.broker.getvalue()\n",
    "        \n",
    "        #SOD\n",
    "        if self.lastDay!=dt.day:\n",
    "            self.lastDay=dt.day\n",
    "            #print(\"[%s] SOD:cash=%.2f\" % (dt,self.broker.getvalue()))"
   ]
  },
  {
   "source": [
    "Define the trading strategy that we want to backtest. The method next defines the trade logic at each timeseries record."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStrategy(StrategyTemplate):\n",
    "\n",
    "    def __init__(self):  # Initiation\n",
    "        super(MyStrategy, self).__init__()\n",
    "        \n",
    "    def init_broker(broker):\n",
    "        broker.setcash(1000000.0)\n",
    "        broker.setcommission(commission=0.0) \n",
    "        \n",
    "    def add_data(cerebro):\n",
    "        data = MyFeed()\n",
    "        cerebro.adddata(data)\n",
    "        return data\n",
    "    \n",
    "    def next(self):  # Processing\n",
    "        super(MyStrategy, self).next()\n",
    "        dt=self.datas[0].datetime.datetime(0)\n",
    "        r=self.data.m[dt]\n",
    "        print(r)\n",
    "        size=self.cerebro.strat_params['size']\n",
    "        threshold_PctChg=self.cerebro.strat_params['pct_chg']\n",
    "               \n",
    "        model=self.cerebro.strat_params['model']\n",
    "        df=spark.createDataFrame([r])\n",
    "        closePrice=r['close']\n",
    "        predicedPrice = model.transform(df).collect()[0]['prediction']\n",
    "        expectedPctChg=(predicedPrice-closePrice)/closePrice*100.0\n",
    "        \n",
    "        goLong=expectedPctChg>threshold_PctChg\n",
    "        goShort=expectedPctChg<-threshold_PctChg\n",
    "        print(\"expectedPctChg=%s,goLong=%s,goShort=%s\" % (expectedPctChg,goLong,goShort))\n",
    "        \n",
    "        if not self.position:\n",
    "            if goLong:\n",
    "                self.buy(size=size) # Go long\n",
    "            else:\n",
    "                self.sell(size=size) # Go short\n",
    "        elif self.position.size>0 and goShort:\n",
    "            self.sell(size=size*2)\n",
    "        elif self.position.size<0 and goLong:          \n",
    "            self.buy(size=size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create what-if scenarios and run trading strategies with different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scenarios\n",
    "scenarios=[]\n",
    "for p in range(0,len(models)):\n",
    "    for s in range(0,1):\n",
    "        c={\"size\":100,\"pct_chg\":0.10*s,\"model\":models[p]}\n",
    "    scenarios.append(c)\n",
    "print(scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run scenarios\n",
    "best_config=None\n",
    "best_pnl=None\n",
    "n=0\n",
    "for c in scenarios:\n",
    "    print(\"*** [%s] RUN SCENARIO:%s\" % ((n+1),c))\n",
    "    config=c\n",
    "    algo=AlgoStrategy(MyStrategy)\n",
    "    algo.cerebro.strat_params=config\n",
    "    algo.run()\n",
    "    if best_pnl is None or best_pnl<algo.pnl:\n",
    "        best_config=c\n",
    "        best_pnl=algo.pnl\n",
    "    n+=1\n",
    "        \n",
    "# best scenario\n",
    "print(\"*** BEST SCENARIO:pnl=%s,config=%s\" % (best_pnl,best_config))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "FinSpace PySpark (finspace-sparkmagic-f9c1f/latest)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}